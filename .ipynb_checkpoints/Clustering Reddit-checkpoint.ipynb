{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projet de Données à la Connaissance\n",
    "======\n",
    "\n",
    "## Introduction\n",
    "\n",
    "## Données\n",
    "\n",
    "### Origine\n",
    "dataset : les posts et commentaires écrits sur Reddit en février 2017\n",
    "\n",
    "### Prétraitement\n",
    "\n",
    "## Choix de l'algorithme de clustering\n",
    "DBSCAN \n",
    "raison du choix :\n",
    "- Non-flat geometry, \n",
    "- uneven cluster sizes\n",
    "- Très scalable\n",
    "- Nombre de clusters inconnus (vs K-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Ouverture des données\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ouverture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts=pd.read_csv(\"data/reddit_posts.csv\",sep=\";\", names=['Id','UserName','SubReddit','Title','Body'], header=None)\n",
    "df_comments=pd.read_csv(\"data/reddit_comments.csv\",sep=\";\", names=['UserName','SubReddit','Comment'], header=None,chunksize=10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>UserName</th>\n",
       "      <th>SubReddit</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>SRASC</td>\n",
       "      <td>Honda</td>\n",
       "      <td>The new S2000?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>JRabone</td>\n",
       "      <td>Patriots</td>\n",
       "      <td>I made a video for SB LI supporting our Pats!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>adamorn</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>Trump supporters - Do you feel that he is doin...</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>JakeM917</td>\n",
       "      <td>doctorwho</td>\n",
       "      <td>The 13th Doctor we deserve</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>DataScienceInc</td>\n",
       "      <td>pystats</td>\n",
       "      <td>Introduction to Correlation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id        UserName  SubReddit  \\\n",
       "0   1           SRASC      Honda   \n",
       "1   3         JRabone   Patriots   \n",
       "2   4         adamorn  AskReddit   \n",
       "3   5        JakeM917  doctorwho   \n",
       "4  10  DataScienceInc    pystats   \n",
       "\n",
       "                                               Title       Body  \n",
       "0                                     The new S2000?        NaN  \n",
       "1      I made a video for SB LI supporting our Pats!        NaN  \n",
       "2  Trump supporters - Do you feel that he is doin...  [removed]  \n",
       "3                         The 13th Doctor we deserve        NaN  \n",
       "4                        Introduction to Correlation        NaN  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts.head()\n",
    "#df_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Première approche : Clustering des SubReddits par NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mathieu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import gensim # Pour lire word2vec\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aren't\n",
      "['Are', \"n't\"]\n",
      "['Aren', 't']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "word=\"Aren't\"\n",
    "word=word.replace(r\"http\\S+\", \"\")\n",
    "word = word.replace(r\"@\\S+\", \"\")\n",
    "word = word.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "word = word.replace(r\"@\", \"at\")\n",
    "word = word.lower()\n",
    "print(word)\n",
    "token_0 = nltk.word_tokenize(\"Aren't\")\n",
    "print(token_0)\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "token_1=tokenizer.tokenize(\"Aren't\")\n",
    "print(token_1)\n",
    "print((word2vec[token_0[0]]==word2vec[token_1[0]]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TextFileReader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-af4e191b5ed6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_posts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SubReddit'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_comments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SubReddit'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Comment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'TextFileReader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "result = pd.concat(df_posts[['SubReddit','Title','Body']], df_comments[['SubReddit','Comment']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deuxième approche : Clustering des SubReddits par Utilisateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison des clusters obtenus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
